---
layout: single
title:  "How to chat with your local ChatGPT?"
date:   2025-07-14 16:00:10 +0800
categories:
  - CS
  - Skills
tags:
  - Ollama
  - Gemma
---

### How to chat with your local ChatGPT?

AIs like ChatGPT have becoming increasingly inseparatable for our local lives. Everytime platforms like ChatGPT or Claude went down, or our internet connection became unstable, our productivity drop dramatically and we cannot continue to work. With **Ollama**, you will be free from such panic and enjoy ChatGPT **on your own computer** even without internect connection!

#### 0. Prerequest

- Laptop with at least **16GB RAM** (For my Macbook Pro 1 with 16GB RAM, it is easy to run **Gemma3 4b**)
- Access to [GitHub](https://github.com/)

#### 1. Download Ollama

- Go to [Ollama](https://ollama.com/) and download the latest version.
- Once you finished download, you do not have any AI on your computer yet. So you have to download the one you want by yourself.

#### 2. Download AI

- Open your **Terminal**.
- Run the following command:
```bash
ollama pull gemma3:1b
```
- This command will doanload **Gemma3 1b**, which is basically the lightest AI.
- For more models please check Ollama document [here](https://github.com/ollama/ollama#quickstart).
> **Note**: The bigger the size is, i.e. the number before `b`, the more powerful the model is. If your small model cannot handle your request, maybe it is because you feed it too many stuff and it cannot handle. Try with a larger one.

#### 3. Run AI

- Open your **Terminal**.
- Run the following command:
```bash
ollama run gemma3:1b
```

#### 4. Setup Web UI (Optional)

If you want to use an **interface** just like ChatGPT, you may choose [Open-WebUI](https://github.com/open-webui/open-webui), which is the user interface supported for Ollama.

##### 4.1 Install Docker
- Go to [Docker](https://www.docker.com/) and download their latest verion.

##### 4.2 Set up Open-WebUI

- Open your **Terminal**
- Run the command
```bash
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```
> **Note**: Everytime you need to open the docker container if using Open-WebUI.

##### 4.3 Open the Website

- Go to your browser and open `http://localhost:3000/`

Now let's chat on your own computer!